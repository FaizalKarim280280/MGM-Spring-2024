{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491d3b48-0f56-4380-a61a-a20ba89e65ab",
   "metadata": {},
   "source": [
    "# MGM Assignment (IIIT-H): Assignment on VQ-VAE for image generation. #\n",
    "\n",
    "## Pawan Kumar, IIIT-H, Hyderabad, India\n",
    "\n",
    "## Instructions\n",
    "* Write answers to questions in this notebook\n",
    "* You may run this on google colab\n",
    "* Using GPUs ir recommended\n",
    "* Use markdown, when answering: create cell, ESC+M to create a markdown cell, you may use $$ to write formula, etc\n",
    "* Submit this notebook, and figures generated in one zip file, the name of file is your roll number\n",
    "* Feel free do more than what is asked, for example, on more extensive runs, etc\n",
    "* You mostly need to run, generate figures and write observations in this assignment\n",
    "\n",
    "The **Vector Quantized Variational Autoencoder (VQ-VAE)** is a powerful approach to deep learning that builds upon the traditional variational autoencoder (VAE) framework. Introduced by researchers at DeepMind, VQ-VAE leverages the principles of vector quantization to enhance the efficiency and quality of latent space representations, making it particularly useful for tasks like image and audio processing.\n",
    "\n",
    "### Background on VAEs\n",
    "\n",
    "Traditional VAEs are generative models that learn to encode input data into a continuous latent space and then decode from this space to reconstruct the input. The VAE framework is grounded in probabilistic graphical models where it attempts to maximize the likelihood of the data while regularizing the encoded representations to adhere to a prior distribution, typically a Gaussian. This helps in generating new data points and manipulating the input data in meaningful ways.\n",
    "\n",
    "### Introduction of Vector Quantization\n",
    "\n",
    "Vector Quantization (VQ) introduces a discrete rather than continuous latent representation by mapping the encoder outputs to a finite set of vectors in a high-dimensional space. Each vector, often called a \"code\" or \"embedding,\" represents a cluster center in the latent space, and each input is encoded to the nearest vector in this set. This quantization step turns the encoder's output into a categorical variable, significantly affecting the model's architecture and the nature of the latent space.\n",
    "\n",
    "### Key Features of VQ-VAE\n",
    "\n",
    "1. **Discrete Representation**: Unlike traditional VAEs that operate in a continuous latent space, VQ-VAE's use of vector quantization creates a discrete latent representation. This has shown to improve the model's robustness and the interpretability of the latent space.\n",
    "\n",
    "2. **High-Quality Reconstruction**: By using a discrete latent space, VQ-VAE can achieve sharper and more coherent reconstructions than standard VAEs, which sometimes produce blurry results. This property is particularly valuable in applications like super-resolution and text-to-speech synthesis.\n",
    "\n",
    "3. **Stability in Training**: Vector quantization helps stabilize the training of autoencoders by reducing the issue known as \"posterior collapse,\" where the model ignores the latent code.\n",
    "\n",
    "4. **Efficiency in Encoding**: VQ-VAE models are efficient in encoding information, making them suitable for compressing high-dimensional data like images and audio into a more manageable, compact format.\n",
    "\n",
    "### Applications\n",
    "\n",
    "VQ-VAE has been successfully applied in various domains, including but not limited to, image generation, speech synthesis, and music generation. It allows for effective style transfer, anomaly detection in images, and generating high-quality synthetic data.\n",
    "\n",
    "**Question:** List five papers that use VQ-VAE or some modifications of it.\n",
    "\n",
    "Overall, VQ-VAE represents a significant advancement in the field of autoencoders, combining the strengths of variational inference with the practical benefits of vector quantization to tackle a broad spectrum of challenges in machine learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd2d9d-ac54-4e04-a226-f72b9742efa1",
   "metadata": {},
   "source": [
    "# Installing libraries #\n",
    "The command `!pip install torch torchvision matplotlib numpy scipy six umap-learn` below is a typical usage example of `pip`, the Python package installer. This command is used to install several libraries in one go, and it's often used within Python environments like Jupyter notebooks (where the `!` symbol allows you to run system shell commands). Here's a breakdown of each library mentioned in the command and what it's commonly used for:\n",
    "\n",
    "1. **`torch`**: This is the PyTorch library, a popular open-source machine learning library developed by Facebook's AI Research lab. It's widely used for applications such as natural language processing and computer vision, particularly because it provides strong support for deep learning models and tensor computations with GPU acceleration.\n",
    "\n",
    "2. **`torchvision`**: A companion package to PyTorch that specifically targets computer vision tasks. It provides tools for loading and preprocessing image data as well as pre-trained models built with PyTorch that can be used for tasks like image classification, object detection, etc.\n",
    "\n",
    "3. **`matplotlib`**: A plotting library for Python and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. It's very useful for visualizing data and model results.\n",
    "\n",
    "4. **`numpy`**: Stands for Numerical Python, it is a fundamental package for scientific computing with Python. It supports a powerful N-dimensional array object and provides tools for integrating C, C++, and Fortran code. It's widely used for large-scale numerical computations.\n",
    "\n",
    "5. **`scipy`**: Built on NumPy, SciPy is a library that contains various tools for scientific computing including linear algebra, optimization, integration, and statistics. SciPy is more focused on technical computing than numerical code for generic programming.\n",
    "\n",
    "6. **`six`**: This is a Python 2 and 3 compatibility library. It provides utility functions for writing code that is portable across Python versions. This is especially useful for maintaining projects that need to run on both Python 2 and Python 3.\n",
    "\n",
    "7. **`umap-learn`**: This package implements Uniform Manifold Approximation and Projection (UMAP), a machine learning algorithm for dimensionality reduction. It is comparable to t-SNE for visualizing clusters or groups of data points, but often faster and more scalable.\n",
    "\n",
    "When you execute this command in a Python environment, `pip` will download and install these packages from the Python Package Index (PyPI), allowing you to use them in your Python code. This is particularly useful when setting up a new environment or ensuring that all necessary dependencies are installed for a specific project or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e19d07-992d-4abf-9aeb-947627dd156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib numpy scipy six umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f4ce74-f46c-4167-a3ea-2d4cda62f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc6ea9-df3d-4df1-8dbc-3163be9e26d1",
   "metadata": {},
   "source": [
    "The code snippet `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` below is used in PyTorch, a popular deep learning framework, to dynamically set the device for computation based on the availability of a GPU. This allows for more efficient execution of deep learning models where a GPU is available. Here's a breakdown of each component:\n",
    "\n",
    "1. **`torch.device`**: This is a PyTorch function that returns a device object which can be either a CPU or a GPU. This object can be used to specify where tensors should be allocated.\n",
    "\n",
    "2. **`\"cuda\"`**: This is the string identifier for a CUDA device. CUDA is a parallel computing platform and application programming interface model created by Nvidia. Using CUDA allows PyTorch to utilize Nvidia's GPU computing capabilities for substantial speedups in matrix operations, which are central to deep learning.\n",
    "\n",
    "3. **`torch.cuda.is_available()`**: This function checks if CUDA (GPU support) is available on the system. If a CUDA-capable GPU is present and configured correctly with the necessary drivers and CUDA toolkit, this function will return `True`. \n",
    "\n",
    "4. **`\"cpu\"`**: This is the string identifier for the central processing unit (CPU) of a computer. If no suitable GPU is available, the operations default to the CPU.\n",
    "\n",
    "5. **Conditional Statement (`\"cuda\" if torch.cuda.is_available() else \"cpu\"`)**: This Python ternary conditional operator selects `\"cuda\"` if `torch.cuda.is_available()` returns `True`, indicating a GPU is available and can be used. Otherwise, it falls back to `\"cpu\"`.\n",
    "\n",
    "By assigning the result to the variable `device`, you can then use this device object in your code to ensure that all tensors and computations are placed on the right hardware. For example, you can send a tensor to the chosen device by calling `tensor.to(device)`, which would automatically place it on a GPU if available, or on the CPU otherwise.\n",
    "\n",
    "This practice is essential for writing flexible, hardware-agnostic PyTorch code that can run on different environments and platforms, optimizing performance when possible by leveraging GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb505233-206d-4a44-8316-397a180d2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68276800-5577-4862-9eab-9c0cbfbdd0db",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The code snippet below sets up the training and validation datasets for a machine learning model using the CIFAR-10 dataset in PyTorch. Here’s a detailed breakdown of each component of this code:\n",
    "\n",
    "### CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset is a well-known public dataset used for machine learning research. It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images.\n",
    "\n",
    "### PyTorch `datasets.CIFAR10`\n",
    "\n",
    "`datasets.CIFAR10` is a function provided by PyTorch that allows easy downloading and loading of the CIFAR-10 dataset. It has several parameters to control how the data is handled:\n",
    "\n",
    "- **`root`**: This is the path to the directory where the dataset will be stored. Here, `\"data\"` means the dataset will be saved in a folder named `data` relative to the current working directory.\n",
    "\n",
    "- **`train`**: A boolean that specifies whether to load the training subset (`True`) or the test subset (`False`). For the training data, `train=True` is used, and for validation data, `train=False`.\n",
    "\n",
    "- **`download`**: If `True`, the dataset will be downloaded from the internet if it's not available at the `root` directory.\n",
    "\n",
    "- **`transform`**: This parameter is used to specify a series of transformations (processing steps) that are applied to the images when they are loaded. Transformations are crucial for normalizing and preparing raw image data for machine learning models.\n",
    "\n",
    "### Transforms\n",
    "\n",
    "The `transforms.Compose` function combines several transformations into a single operation. Here, two transformations are applied to both training and validation data:\n",
    "\n",
    "1. **`transforms.ToTensor()`**: This converts the images from Python Image Library format to PyTorch tensors and scales the image's pixel intensity values from 0-255 to 0-1. This is essential because neural networks typically perform better with input data normalized to a standard scale.\n",
    "\n",
    "2. **`transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))`**: This normalizes the tensor image data. The first tuple `(0.5, 0.5, 0.5)` specifies the mean for each channel (Red, Green, Blue) to be subtracted, and the second tuple `(1.0, 1.0, 1.0)` specifies the standard deviation for each channel to divide. Given the division by 1.0, this transform essentially shifts the range from [0,1] to [-0.5,0.5] for each channel.\n",
    "\n",
    "### Practical Usage\n",
    "\n",
    "- **Training Data**: The variable `training_data` holds the training set, which includes 50,000 images. These are used to train the model.\n",
    "  \n",
    "- **Validation Data**: The variable `validation_data` holds the test set, which consists of 10,000 images. These are used to evaluate the performance of the model on unseen data, helping to assess the model's generalizability.\n",
    "\n",
    "By setting up `training_data` and `validation_data` this way, you can easily load and preprocess the data for use in training and evaluating a deep learning model, leveraging PyTorch's efficient data handling and transformation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f9c87-9d30-443d-8b7c-353d7873ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "validation_data = datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3e78a-bcff-4b16-9317-584e859f72af",
   "metadata": {},
   "source": [
    "The line `data_variance = np.var(training_data.data / 255.0)` in Python, using NumPy, computes the variance of the training data for the CIFAR-10 dataset. Let's break down each component of this computation:\n",
    "\n",
    "1. **`training_data.data`**: This part of the code accesses the `data` attribute of the `training_data` object, which contains the CIFAR-10 training images. Typically, in PyTorch datasets, this attribute holds the raw image data as a NumPy array where the pixel values are integers ranging from 0 to 255. Each entry in this array represents a pixel's intensity across its three color channels (Red, Green, Blue).\n",
    "\n",
    "2. **`/ 255.0`**: This operation is called normalization and is used to scale the pixel values to a range of 0 to 1. Dividing by 255.0 (the maximum possible pixel value for an 8-bit image) changes the data type to float and scales the data accordingly. Normalizing data is important for many machine learning algorithms because it ensures that the input features (in this case, pixel values) are on a similar scale, which helps the algorithm converge more quickly and efficiently during training.\n",
    "\n",
    "3. **`np.var()`**: This function calculates the variance of an array. Variance measures how spread out the values in a dataset are around the mean. It's a crucial statistical measure that indicates the variability of the dataset. In the context of image data, a higher variance usually indicates more diverse pixel values and potentially more complex textures and patterns in the images.\n",
    "\n",
    "   - **Variance Formula**: The variance \\(\\sigma^2\\) is computed as \\(\\sigma^2 = \\frac{1}{N} \\sum (x_i - \\mu)^2\\), where \\(x_i\\) are the data points, \\(\\mu\\) is the mean of the data, and \\(N\\) is the number of data points.\n",
    "\n",
    "4. **`data_variance`**: This variable stores the computed variance value, which might be used later in the training process or in preprocessing steps. For instance, understanding the variance can help in configuring neural network layers or in further normalization steps, like feature scaling or batch normalization.\n",
    "\n",
    "The calculated variance of the training data can be used to inform the preprocessing and training of a machine learning model, helping to tune the model or adjust its learning parameters to improve performance. Understanding the spread of pixel values across a dataset is essential for developing effective models, particularly in tasks that involve image recognition or classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a223c463-0af3-435b-b4df-b8bb3939f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variance = np.var(training_data.data / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb288df0-0fe6-4b3f-852f-bb4f765620db",
   "metadata": {},
   "source": [
    "# Vector Quantizer Module\n",
    "\n",
    "The code snippet below is a Python class implementation of a Vector Quantizer module, typically used as part of a Vector Quantized Variational Autoencoder (VQ-VAE). This class is built using PyTorch, a popular deep learning framework. Let’s walk through the code to understand how it works:\n",
    "\n",
    "### Class Definition and Initialization\n",
    "- **`VectorQuantizer(nn.Module)`**: The class `VectorQuantizer` inherits from `nn.Module`, which is the base class for all neural network modules in PyTorch. This inclusion enables easy integration of this module into larger neural network architectures and provides access to many utility functions and hooks.\n",
    "\n",
    "- **`__init__(self, num_embeddings, embedding_dim, commitment_cost)`**: This is the constructor for the `VectorQuantizer` class. It initializes the module with three key parameters:\n",
    "  - `num_embeddings`: The number of discrete embeddings (or codes) in the quantization dictionary.\n",
    "  - `embedding_dim`: The dimensionality of each embedding vector.\n",
    "  - `commitment_cost`: A scalar that controls the weighting of the loss term that encourages the input to stay close to the quantized output (more on this in class lecture).\n",
    "\n",
    "- **Embedding Initialization**: The embeddings are stored in an `nn.Embedding` layer, which is initialized with weights uniformly distributed over a small range derived from the number of embeddings. This layer will learn the optimal representations of data points in the embedding space during training.\n",
    "\n",
    "### Forward Pass\n",
    "- **Input Transformation**: The input tensor typically has the shape `[batch size, channels, height, width]` (BCHW). It’s first permuted to `[batch size, height, width, channels]` (BHWC) to facilitate operations on the channel dimensions, which aligns with the embedding dimension.\n",
    "\n",
    "- **Flatten Input**: The input is reshaped into a two-dimensional tensor where each row represents a pixel or small patch from the input, prepared for comparison against the embedding vectors.\n",
    "\n",
    "- **Distance Calculation**: The distances between each input vector and all embeddings are calculated using an efficient vectorized formula that computes the squared Euclidean distance.\n",
    "\n",
    "- **Encoding and Quantization**:\n",
    "  - The closest embedding for each input vector is identified (`encoding_indices`), and a one-hot encoded tensor of these indices is created (`encodings`).\n",
    "  - The quantized output is then computed by multiplying these encodings with the embedding weights, effectively replacing each input vector with its nearest embedding vector.\n",
    "\n",
    "- **Loss Calculation**:\n",
    "  - `e_latent_loss`: The MSE loss between the detached (non-gradient-flowing) quantized output and the original input, which represents the error introduced by the quantization process.\n",
    "  - `q_latent_loss`: The MSE loss between the quantized output (where gradients are allowed to flow) and a detached copy of the input, used to update the embeddings.\n",
    "  - The total loss combines these two components, with the commitment cost scaled `e_latent_loss`.\n",
    "\n",
    "- **Gradient Control**: The line `quantized = inputs + (quantized - inputs).detach()` is crucial as it adjusts the quantized output to be closer to the input while stopping gradients from flowing through the original quantization operation, focusing model updates on improving the embeddings.\n",
    "\n",
    "- **Perplexity Calculation**: This metric measures how evenly the different embeddings are being used (a higher perplexity indicates more uniform usage), which can be important for diagnosing model training dynamics.\n",
    "\n",
    "- **Output Transformation and Return**: Finally, the output tensor is permuted back to the original BCHW format (**Quiz:** Which line in the code does this?). The method returns several items including the loss, quantized tensor, perplexity, and the one-hot encodings for potential further analysis or use in the model.\n",
    "\n",
    "This implementation encapsulates the core mechanics of vector quantization within VQ-VAE models, focusing on efficient data representation and learning discrete latent spaces that are useful in various applications such as image compression and generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907e68d8-bd0f-4039-81f6-0d40cca50af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266cecc3-2bb3-4801-b291-c30c12d8aee7",
   "metadata": {},
   "source": [
    "# Another Vector Quantizer \n",
    "\n",
    "The `VectorQuantizerEMA` class is another implementation of a Vector Quantizer, similar to the `VectorQuantizer` module discussed earlier but with a key difference in how it updates the embedding vectors. The `EMA` in the class name stands for Exponential Moving Average, which this class uses to update the embeddings instead of the straightforward gradient descent method used in the `VectorQuantizer` class. Let's go through the components and highlight the differences:\n",
    "\n",
    "### Class Definition and Initialization\n",
    "- The constructor initializes with additional parameters `decay` and `epsilon`, which are specific to the EMA updating mechanism:\n",
    "  - **`decay`**: This controls the rate at which old observations are down-weighted compared to new data.\n",
    "  - **`epsilon`**: A small constant (usually `1e-5`) added to prevent division by zero and stabilize calculations.\n",
    "\n",
    "### Exponential Moving Average (EMA) Mechanics\n",
    "- **EMA Buffers and Parameters**: Two new properties are initialized for managing the EMA:\n",
    "  - **`_ema_cluster_size`**: This buffer tracks the size (i.e., the count of data points) of each cluster as a moving average.\n",
    "  - **`_ema_w`**: This is a parameter that represents the weighted sum of the data points (embeddings) as a moving average.\n",
    "\n",
    "- These buffers are initialized and then used during the training process to update the embeddings based on the incoming data, smoothed over previous updates.\n",
    "\n",
    "### Forward Pass Modifications\n",
    "- **Embedding Updates Using EMA**:\n",
    "  - During the forward pass, after calculating which embedding each piece of input data is closest to (the encoding), the `_ema_cluster_size` and `_ema_w` are updated:\n",
    "    - `_ema_cluster_size` is updated using a decay factor to slowly forget older observations.\n",
    "    - `_ema_w` accumulates the sum of the input vectors, weighted by their respective encodings, also factoring in the decay.\n",
    "  - The actual embedding weights (`_embedding.weight`) are then recalculated as the ratio of `_ema_w` to `_ema_cluster_size`, which effectively means each embedding vector is the moving average of the inputs assigned to it, normalized by the smoothed count of assignments.\n",
    "\n",
    "### Loss Computation and Other Outputs\n",
    "- The loss calculation is simplified compared to the previous module, focusing only on the commitment loss (scaled by `commitment_cost`), which is the mean squared error between the quantized outputs and the inputs.\n",
    "- The straight-through estimator step remains the same, ensuring that gradients can flow through the quantization step by bypassing the non-differentiable operations. (**Quiz: Which line in the code corresponds to this?**)\n",
    "- The output also includes perplexity calculations, similar to before, providing a measure of how evenly the embeddings are being utilized.\n",
    "\n",
    "### Key Differences\n",
    "1. **Embedding Updates**: The primary difference is in how the embedding vectors are updated. The `VectorQuantizerEMA` uses exponential moving averages, which can lead to more stable updates because it incorporates the history of assignments rather than just the current batch. This can be particularly beneficial in environments with noisy data or when data distributions change over time.\n",
    "2. **Loss Calculation**: The `VectorQuantizerEMA` omits the direct gradient of the quantization loss (`q_latent_loss` from the first class) in the loss function, potentially simplifying the gradient landscape.\n",
    "\n",
    "Overall, `VectorQuantizerEMA` offers a method that might be more robust to variations in data input, making it potentially more suitable for real-world applications where data distributions are not static. **Question: Verify this claim by comparing with VectorQuantizer and upload figures.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de9d763-1fce-4569-8fe9-1fca11adace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4990b41-172b-4991-8993-ecadd7e390bf",
   "metadata": {},
   "source": [
    "# Residual Blocks\n",
    "\n",
    "The provided code defines two Python classes, `Residual` and `ResidualStack`, using PyTorch's deep learning framework. These classes are structured to create residual blocks and a stack of these blocks, respectively, which are commonly used in deep learning architectures to enhance training of deep neural networks by enabling effective backpropagation of gradients. \n",
    "\n",
    "Recall the class lectures that residual blocks and similar ideas were proposed in these references: \n",
    "\n",
    "* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "* [Highway Networks](https://arxiv.org/abs/1505.00387)\n",
    "* [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "Ofcourse, see lecture slides for extensive review of these and more references. \n",
    "\n",
    "Let's explore the residual block defined below in detail:\n",
    "\n",
    "### Class `Residual`\n",
    "This class implements a single residual block, which is a fundamental component in modern neural network architectures, particularly **useful in constructing deep networks**:\n",
    "\n",
    "- **Initialization (`__init__`)**:\n",
    "  - `in_channels`: Number of channels in the input feature map.\n",
    "  - `num_hiddens`: Number of channels in the output feature map of the block.\n",
    "  - `num_residual_hiddens`: Number of channels in the intermediate layers within the residual block.\n",
    "\n",
    "  The residual block consists of two convolutional layers:\n",
    "  1. The first `nn.Conv2d` layer increases (or decreases) the depth of the input feature map from `in_channels` to `num_residual_hiddens` and uses a 3x3 kernel with padding of 1. This ensures that the output of this layer has the same spatial dimensions as the input.\n",
    "  2. The second `nn.Conv2d` layer adjusts the depth from `num_residual_hiddens` back to `num_hiddens`, using a 1x1 kernel, effectively allowing the network to learn an appropriate channel-wise transformation.\n",
    "\n",
    "  Between and after these convolutional layers, ReLU activations are applied to introduce non-linearities into the model.\n",
    "\n",
    "- **Forward Pass (`forward`)**:\n",
    "  - The residual block computes the output by adding the input `x` directly to the output of the convolutional block (`self._block(x)`). This addition is the \"shortcut connection\" that defines a residual block, allowing gradients to flow directly through the network during backpropagation.\n",
    "\n",
    "### Class `ResidualStack`\n",
    "This class aggregates multiple `Residual` blocks into a cohesive module, often used in deeper network architectures:\n",
    "\n",
    "- **Initialization (`__init__`)**:\n",
    "  - `in_channels`: The number of input channels to the first residual block.\n",
    "  - `num_hiddens`: The number of output channels for the last convolution in each residual block.\n",
    "  - `num_residual_layers`: The number of residual blocks to be stacked.\n",
    "  - `num_residual_hiddens`: The number of channels for the intermediate convolution within each residual block.\n",
    "\n",
    "  It constructs a list (`nn.ModuleList`) of `Residual` blocks. Each block in this list is instantiated with the same number of input channels, hidden output channels, and residual hidden channels.\n",
    "\n",
    "- **Forward Pass (`forward`)**:\n",
    "  - The method sequentially applies each residual block in the stack to the input `x`, where each block's output is used as the next block's input. This creates a deep stack of residual blocks.\n",
    "  - After all blocks have been applied, a final ReLU activation is applied. This is often done to ensure non-negativity of the output, which can be beneficial for many tasks in deep learning.\n",
    "\n",
    "### Usage in VQ-VAE\n",
    "In the context of VQ-VAE (Vector Quantized Variational AutoEncoder), residual blocks and stacks can be used in both the encoder and decoder parts:\n",
    "- **Encoder**: Helps in effectively compressing the input into a compact latent representation.\n",
    "- **Decoder**: Assists in reconstructing the input from the quantized latent representation.\n",
    "\n",
    "Using residual blocks helps mitigate the vanishing gradient problem, enabling deeper architectures by ensuring that the signal (and thus the gradient during training) does not diminish as it passes through many layers. This is particularly crucial in VQ-VAE, where accurate and detailed reconstruction of the input is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77a938b-e476-4b40-98a5-29e8654978ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super(Residual, self).__init__()\n",
    "        self._block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=num_residual_hiddens,\n",
    "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
    "                      out_channels=num_hiddens,\n",
    "                      kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self._block(x)\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
    "                             for _ in range(self._num_residual_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_residual_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63498702-a5be-46a2-b469-9929173941c5",
   "metadata": {},
   "source": [
    "# Encoder Module of VQ-VAE\n",
    "\n",
    "The `Encoder` class defined in the code snippet is a specialized neural network module using PyTorch, designed to encode input data (typically images) into a compressed latent representation. This type of encoder is often used in architectures like autoencoders, including Vector Quantized Variational AutoEncoders (VQ-VAE), for tasks such as image compression, noise reduction, and feature extraction. Here’s a detailed breakdown of each component:\n",
    "\n",
    "### Class Definition\n",
    "The `Encoder` class extends `nn.Module`, making it a custom module in PyTorch. It's tailored to take an input with a specific number of channels and process it through multiple layers to produce a condensed feature map.\n",
    "\n",
    "### Constructor (`__init__`)\n",
    "The constructor initializes the encoder with several layers and configurations:\n",
    "- **Parameters**:\n",
    "  - `in_channels`: The number of channels in the input images (e.g., 3 for RGB images).\n",
    "  - `num_hiddens`: The number of channels (features) in the hidden layers. This parameter influences the capacity of the encoder.\n",
    "  - `num_residual_layers`: The number of residual blocks in the residual stack.\n",
    "  - `num_residual_hiddens`: The number of channels in the intermediate layers within each residual block.\n",
    "\n",
    "- **Convolutional Layers**:\n",
    "  - `_conv_1`: The first convolutional layer reduces the spatial dimensions of the input by half (due to `stride=2`) and reduces the channel depth from `in_channels` to `num_hiddens//2`. The kernel size of 4x4 and padding of 1 are typical for capturing spatial hierarchies while downsampling.\n",
    "  - `_conv_2`: The second convolutional layer further reduces the spatial dimensions by half and increases the channel depth from `num_hiddens//2` to `num_hiddens`. This layer continues to compress and encode higher-level features from the input.\n",
    "  - `_conv_3`: The third convolutional layer keeps the spatial dimensions the same (due to `stride=1`) and maintains the channel depth, processing the features further with a smaller 3x3 kernel.\n",
    "\n",
    "- **Residual Stack**:\n",
    "  - `_residual_stack`: An instance of the `ResidualStack` class, which processes the output of the last convolutional layer through a series of residual blocks. This stack helps in building a deep network while mitigating the vanishing gradient problem, allowing the network to learn more complex patterns without losing significant information through the depth.\n",
    "\n",
    "### Forward Pass (`forward`)\n",
    "- The `forward` method defines how the input data flows through the encoder:\n",
    "  - The input is first processed by `_conv_1`, followed by a ReLU activation function (`F.relu`), which introduces non-linearity to the model and helps capture complex patterns in the data.\n",
    "  - The output from the first layer is then fed into `_conv_2`, followed again by a ReLU activation.\n",
    "  - The output of `_conv_2` passes through `_conv_3`, which prepares the data for the residual stack by adjusting the feature map without changing its size.\n",
    "  - Finally, the output of `_conv_3` is passed through the `_residual_stack`, where it undergoes further transformation by the multiple residual blocks.\n",
    "\n",
    "### Purpose and Usage\n",
    "In the context of VQ-VAE, the `Encoder` plays a crucial role in compressing the input image into a dense representation, which is then quantized and reconstructed by the decoder. The use of convolutional and residual layers enables the encoder to efficiently capture both local and global features in the input, which are essential for high-quality image reconstruction in the decoder phase. This structured approach to deep learning model design facilitates the training of robust and efficient models capable of handling complex tasks in image and video processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65398f99-b3a6-494a-843d-068e6e3bd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens//2,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1, padding=1)\n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._conv_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._conv_3(x)\n",
    "        return self._residual_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4bfe7-f738-4060-9928-e93313c7e01b",
   "metadata": {},
   "source": [
    "# Decoder of VQ-VAE\n",
    "\n",
    "The `Decoder` class provided below is designed to reconstruct data from a compressed encoding back to its original form, typically used in architectures like autoencoders and, more specifically, Vector Quantized Variational AutoEncoders (VQ-VAE). This class also extends `nn.Module`, making it a custom PyTorch module. Let's explore the structure and functionality of this decoder:\n",
    "\n",
    "### Class Definition\n",
    "- The `Decoder` is initialized with parameters that define the architecture of the network, including the number of channels and layers, similar to how an encoder is configured but working in reverse.\n",
    "\n",
    "### Constructor (`__init__`)\n",
    "- **Parameters**:\n",
    "  - `in_channels`: The number of channels in the input feature map (i.e., the output from the encoder or the latent space).\n",
    "  - `num_hiddens`: The number of channels in the hidden layers, defining the capacity of the decoder.\n",
    "  - `num_residual_layers`: The number of residual blocks within the residual stack.\n",
    "  - `num_residual_hiddens`: The number of channels in the intermediate layers within each residual block.\n",
    "\n",
    "- **Layers**:\n",
    "  - **`_conv_1`**: A convolutional layer that processes the input features without changing their spatial dimensions (due to `stride=1` and `padding=1`). It uses a 3x3 kernel, which is standard for processing feature maps without altering their size.\n",
    "  - **`_residual_stack`**: Similar to the encoder, this is a stack of residual blocks (using the `ResidualStack` class) that further processes the features. The residual stack is crucial for allowing the decoder to effectively learn and represent complex patterns, aiding in accurate reconstruction.\n",
    "  - **`_conv_trans_1`**: A transposed convolutional layer (often called deconvolution), which increases the spatial dimensions of the feature map. It essentially performs the opposite of a convolution by mapping the input feature map to a larger output map, thus starting to upscale the image back to its original size. It reduces the number of channels from `num_hiddens` to `num_hiddens//2`.\n",
    "  - **`_conv_trans_2`**: Another transposed convolutional layer that further upscales the feature map and reduces the channel depth to match the original input's channel count (typically 3 for RGB images), completing the reconstruction of the image.\n",
    "\n",
    "### Forward Pass (`forward`)\n",
    "- The forward pass of the decoder details how the input data flows through it:\n",
    "  1. **Initial Convolution**: The input is first processed by `_conv_1`, which prepares the encoded features for further upscaling.\n",
    "  2. **Residual Processing**: The output from `_conv_1` is passed through the `_residual_stack`, which refines and enriches the feature representations without altering their size.\n",
    "  3. **Upscaling**: The refined features are then passed through `_conv_trans_1`, where they are upscaled and the channel depth is reduced. A ReLU activation follows to introduce non-linearity.\n",
    "  4. **Final Reconstruction**: The output of the first transposed convolution layer is processed by `_conv_trans_2`, which further upscales the features to match the original dimensions of the input image.\n",
    "\n",
    "### Purpose and Usage\n",
    "- In a VQ-VAE, the decoder's role is critical as it must reconstruct high-fidelity output from a quantized, compressed representation of the original input. The quality of reconstruction depends heavily on the design of the decoder, including the effectiveness of its residual blocks and upsampling layers.\n",
    "- By employing residual blocks, the decoder can maintain or enhance feature quality through deeper layers, facilitating better reconstruction. The transposed convolution layers gradually restore the spatial dimensions necessary for achieving outputs that closely resemble the original inputs.\n",
    "\n",
    "This architecture is particularly useful for tasks where lossless or near-lossless reconstruction is crucial, such as in image and video processing applications. The use of transposed convolutions helps in achieving smoother and more detailed outputs compared to simpler upscaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f79b826c-e896-4d75-bc55-507b24d35607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, padding=1)\n",
    "        \n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "        \n",
    "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens//2,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "        \n",
    "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n",
    "                                                out_channels=3,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        \n",
    "        x = self._residual_stack(x)\n",
    "        \n",
    "        x = self._conv_trans_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return self._conv_trans_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b088946-18b7-4c3a-a364-bc8f5632b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_training_updates = 1500\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29a56b-11ad-4bc1-aee2-9283a9ea4306",
   "metadata": {},
   "source": [
    "The code snippet below creates a `DataLoader` in PyTorch, which is a critical component for efficiently managing and iterating over datasets during the training of machine learning models. Here's a breakdown of each component and how they contribute to the data loading and training process:\n",
    "\n",
    "### DataLoader\n",
    "`DataLoader` is a class provided by PyTorch that abstracts the complexity of handling datasets. It offers a convenient way to iterate over datasets with the ability to batch, shuffle, and load the data asynchronously using multiprocessing workers.\n",
    "\n",
    "### Parameters\n",
    "- **`training_data`**: This is the dataset to be loaded. It should be an instance of a dataset class from `torch.utils.data.Dataset` or a custom dataset class that you've defined, which includes methods to get the data items and their labels. In this context, `training_data` might be the CIFAR-10 dataset prepared for training as discussed earlier.\n",
    "\n",
    "- **`batch_size`**: This parameter specifies the number of data items (e.g., images, records) to be processed in each batch. The choice of batch size can affect model performance and training dynamics. A larger batch size can lead to faster processing through parallelism but might require more memory and could potentially lead to a less stable training process in terms of convergence.\n",
    "\n",
    "- **`shuffle`**: When set to `True`, the data loader will shuffle the order of the dataset at the beginning of each epoch. Shuffling is important for training deep learning models as it prevents the model from learning anything about the sequence of the data, reducing the chance of overfitting and helping the model generalize better.\n",
    "\n",
    "- **`pin_memory`**: Setting this parameter to `True` is particularly useful when training models using a GPU. It instructs the data loader to load the dataset into the page-locked (pinned) memory, which enables faster data transfer to CUDA-enabled GPUs. Pinned memory acts as a non-swappable memory area that can be accessed by device drivers and DMA (Direct Memory Access) hardware. This setting can lead to more efficient memory handling, thereby speeding up input data loading.\n",
    "\n",
    "### Usage\n",
    "Once the `DataLoader` is set up with the specified parameters, it can be used in a training loop. Here is a simple example of how it might be used:\n",
    "```python\n",
    "for epoch in range(total_epochs):\n",
    "    for data, labels in training_loader:\n",
    "        # Move the data and labels to device (e.g., GPU)\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients, perform forward pass, compute loss, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "This loop will go through the dataset in batches, automatically handling the shuffling and memory management aspects, thus allowing the model training to focus on learning from the data rather than data handling logistics. We will see how we will use this for VQ-VAE below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45cc53b4-1b9d-41ab-917c-2d07b5ed2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_data, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1998155-b677-4709-9fba-e72ef46a4c32",
   "metadata": {},
   "source": [
    "The `validation_loader` below is another instance of a PyTorch `DataLoader`, specifically set up for handling validation data during the training process of a machine learning model. This is similar to the `training_loader`, but it serves a different purpose. Let's break down the setup and usage of this `DataLoader`:\n",
    "\n",
    "<!-- ### DataLoader\n",
    "PyTorch's `DataLoader` is designed to efficiently manage datasets, providing easy-to-implement options for batching, shuffling, and loading data. It automates the process of generating data batches during model training and validation, which are crucial for evaluating the performance of the model on data it has not been trained on. -->\n",
    "\n",
    "### Parameters\n",
    "- **`validation_data`**: This refers to the dataset used for the model validation. Typically, this dataset is separate from the training dataset and is used to check the model's performance against unseen data. The validation dataset should not influence the training process directly but is used to monitor model generalization and to tune hyperparameters.\n",
    "\n",
    "- **`batch_size=32`**: This specifies that the DataLoader should retrieve 32 data items at a time. This is the size of each batch that will be fed into the model during a validation pass. The choice of batch size can affect computational efficiency and the stability of model evaluation metrics.\n",
    "\n",
    "- **`shuffle=True`**: Although shuffling is generally more critical during training to prevent the model from learning unintended patterns from the order of the data, it is also used here. For validation purposes, shuffling can help in ensuring that the validation results are not biased by any specific order of data, especially if the model might be sensitive to the order in which data batches are presented.\n",
    "\n",
    "- **`pin_memory=True`**: As with the training DataLoader, setting this to `True` helps in speeding up data transfers from CPU to GPU by using page-locked memory. This is especially beneficial when validating on a GPU, as it reduces the data transfer time between the CPU and GPU, making the validation steps run faster.\n",
    "\n",
    "### Usage in Validation\n",
    "The validation DataLoader is typically used in a similar loop structure as the training DataLoader but within a different part of the model training script, often after a training epoch is completed:\n",
    "```python\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Turn off gradients, since we don't need them for validation\n",
    "    for data, labels in validation_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(data)\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        # Accumulate validation loss, calculate validation metrics\n",
    "```\n",
    "In this context, `model.eval()` switches the model to evaluation mode, turning off specific layers like dropout or batch normalization that behave differently during training vs. during testing/validation. The `torch.no_grad()` context manager ensures that the computation graph is not stored (which saves memory and computations).\n",
    "\n",
    "The main goal of using `validation_loader` is to monitor the model's performance on the validation dataset at regular intervals during training, allowing for early stopping or model adjustments before overfitting or underfitting occurs. This setup ensures that the evaluation of the model is efficient and effective, contributing to better overall training outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b27c57-2208-4d53-93ad-ac9189b7f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(validation_data,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80a207-da47-44c6-ae4c-11b95986c268",
   "metadata": {},
   "source": [
    "# Model Class\n",
    "\n",
    "The `Model` class provided below encapsulates a complete architecture for a Vector Quantized Variational AutoEncoder (VQ-VAE), which is a type of generative model that learns to compress data into a discrete latent representation and then reconstructs the input from this representation. As mentioned before, this architecture is particularly useful for tasks where discrete representations are advantageous, such as in generating new content that is similar to the training data, or in applications where compression is required. Here's a breakdown of the components and flow of the `Model` class:\n",
    "\n",
    "### Class Definition\n",
    "- Inherits from `nn.Module`, which is the base class for all neural network modules in PyTorch. This inheritance makes it easy to create custom models with manageable components.\n",
    "\n",
    "### Constructor (`__init__`)\n",
    "- **Parameters**:\n",
    "  - `num_hiddens`: The number of hidden channels in the encoder and decoder networks.\n",
    "  - `num_residual_layers`: The number of residual layers in the encoder and decoder's residual stacks.\n",
    "  - `num_residual_hiddens`: The number of channels in the intermediate layers within each residual block.\n",
    "  - `num_embeddings`: The number of discrete embeddings in the vector quantizer.\n",
    "  - `embedding_dim`: The dimensionality of each embedding vector.\n",
    "  - `commitment_cost`: A weighting factor for the loss term that penalizes the distance between the encoder outputs and the nearest embedding vector, encouraging the encoder to produce outputs close to the existing embeddings.\n",
    "  - `decay`: An optional parameter that, if greater than 0, indicates the model should use the exponential moving average strategy (`VectorQuantizerEMA`) for updating embeddings. Otherwise, it uses the basic `VectorQuantizer`.\n",
    "\n",
    "- **Components**:\n",
    "  - `_encoder`: An instance of the `Encoder` class that processes the input and reduces it to a feature-rich compressed form.\n",
    "  - `_pre_vq_conv`: A convolutional layer that adapts the number of channels from the encoder output to match the embedding dimension required by the vector quantizer.\n",
    "  - `_vq_vae`: Depending on the `decay` parameter, it either initializes a `VectorQuantizerEMA` or a `VectorQuantizer`. This component is crucial for transforming the continuous latent space representation from the encoder into a discrete one, and it also computes part of the model's loss.\n",
    "  - `_decoder`: An instance of the `Decoder` class that takes the quantized (discrete) representation and reconstructs the input data.\n",
    "\n",
    "### Forward Pass (`forward`)\n",
    "- **Flow**:\n",
    "  1. **Encoding**: The input `x` is passed through the `_encoder`, which compresses it into a dense feature representation `z`.\n",
    "  2. **Pre-Vector Quantization Convolution**: The output `z` from the encoder is further processed by `_pre_vq_conv`, adapting its dimensionality to match that required by the vector quantizer.\n",
    "  3. **Vector Quantization**: `z` is then fed into `_vq_vae`, which quantizes it into a discrete form. This step also calculates the vector quantization loss, the output quantized representation, and the perplexity (which measures how well the embedding space is being used).\n",
    "  4. **Decoding**: The quantized representation is decoded by `_decoder` into `x_recon`, which is the reconstruction of the original input.\n",
    "\n",
    "- **Outputs**:\n",
    "  - `loss`: The loss calculated by the vector quantizer, which includes the commitment loss and possibly other components depending on the specifics of the vector quantizer used.\n",
    "  - `x_recon`: The reconstructed input, which is the output of the decoder.\n",
    "  - `perplexity`: A metric that provides insight into the usage of the embedding space (a higher perplexity indicates a more effective and uniform use of the embeddings).\n",
    "\n",
    "### Summary\n",
    "This model is designed to leverage the strengths of convolutional neural networks, residual learning, and vector quantization for efficient data encoding and reconstruction, making it suitable for applications in image and audio processing where maintaining high fidelity is crucial. The architecture also allows for flexibility in the quantization method, which can be crucial for adapting the model to different requirements and data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ddc1644-da5f-4f39-8ced-f5ac82b7f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder(3, num_hiddens,\n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n",
    "                                      out_channels=embedding_dim,\n",
    "                                      kernel_size=1, \n",
    "                                      stride=1)\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = Decoder(embedding_dim,\n",
    "                                num_hiddens, \n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = self._pre_vq_conv(z)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d492c41-629c-4274-b0f9-78eeeca75b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf310f-69fb-4944-9037-125edd551bf2",
   "metadata": {},
   "source": [
    "# Optimizer for Training VQ-VAE Loss\n",
    "\n",
    "The code snippet below initializes an optimizer for training a neural network model in PyTorch, specifically using the Adam optimization algorithm. This optimizer is assigned to manage and update the parameters of the specified `model`. Here's a breakdown of the components and their implications for training the neural network:\n",
    "\n",
    "### Optimizer\n",
    "- **`optim.Adam`**: This is an invocation of the Adam (Adaptive Moment Estimation) optimizer from PyTorch's `optim` module. Adam is a popular optimization algorithm in machine learning because it combines the best properties of the AdaGrad and RMSProp algorithms to handle sparse gradients on noisy problems.\n",
    "\n",
    "### Parameters\n",
    "- **`model.parameters()`**: This function call retrieves all the trainable parameters (weights and biases) from the model. These parameters are what the optimizer will update during the training process.\n",
    "\n",
    "- **`lr=learning_rate`**: This sets the learning rate for the optimizer. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the right learning rate is crucial for good training performance, as a value too small might lead to a long training process, and a value too large might lead to overshooting minima.\n",
    "\n",
    "- **`amsgrad=False`**: This is a parameter specific to the Adam optimizer. `amsgrad` is a variant of the Adam optimizer that modifies the algorithm to provide a theoretical guarantee for convergence. Setting it to `False` uses the original Adam algorithm, which is empirically strong across a variety of tasks. If set to `True`, it can provide more stability in the convergence of the training process at the cost of using more memory and potentially more computation.\n",
    "\n",
    "### Usage\n",
    "The optimizer is used during the training loop to update the weights of the model based on the gradients computed from the loss function. Here’s a simplistic view of how it might be used within a training loop:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "    for inputs, labels in dataloader:  # Loop over each batch from the data loader\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(inputs)  # Forward pass: compute the predicted outputs\n",
    "        loss = loss_function(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass: compute the gradient of the loss with respect to model parameters\n",
    "        optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "```\n",
    "\n",
    "In this loop:\n",
    "- **`optimizer.zero_grad()`** clears old gradients; otherwise, gradients would be accumulated from all forward passes.\n",
    "- **`loss.backward()`** computes the derivative of the loss function with respect to the parameters (or weights) of the model.\n",
    "- **`optimizer.step()`** updates the parameters according to the optimization strategy.\n",
    "\n",
    "Using the Adam optimizer helps in fine-tuning the model efficiently, especially with datasets that have noisy and/or sparse gradients. This optimizer dynamically adapts the learning rate for each parameter and bases updates on the first two moments of the gradients, making it robust across a wide range of tasks and model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4baf878a-d08e-480b-bdac-d62172773363",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29818c-063e-4924-ad81-ca49ff70c462",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "The code below is a training loop for a neural network model in PyTorch, specifically designed for training a model that includes a vector quantization component like a VQ-VAE (Vector Quantized Variational AutoEncoder). This loop is structured to handle batches of data, compute losses, perform backpropagation, and update the model's parameters. Here's a detailed breakdown and explanation of each part of the loop:\n",
    "\n",
    "### Setup for Training\n",
    "- **`model.train()`**: This call sets the model to training mode. This is important because certain layers like dropout or batch normalization behave differently during training than during evaluation.\n",
    "\n",
    "### Data Structures for Tracking Metrics\n",
    "- **`train_res_recon_error`** and **`train_res_perplexity`**: These are lists used to record the reconstruction error and perplexity after each training iteration. Tracking these metrics helps monitor the training process and evaluate the model's performance over time.\n",
    "\n",
    "### Training Loop\n",
    "- **`for i in xrange(num_training_updates):`**: This loop iterates over the training process for a specified number of updates (`num_training_updates`). Note that `xrange` is used in Python 2 and is equivalent to `range` in Python 3.\n",
    "\n",
    "### Inside the Loop\n",
    "1. **Data Loading**:\n",
    "   - **`(data, _) = next(iter(training_loader))`**: This extracts the next batch of data from the `training_loader`. The underscore `_` is used to ignore the second element returned (typically labels, which are not needed for this type of model).\n",
    "   - **`data = data.to(device)`**: Moves the data to a computing device (CPU or GPU). This step is crucial for performance when using GPUs for computation.\n",
    "\n",
    "2. **Gradient Initialization**:\n",
    "   - **`optimizer.zero_grad()`**: Resets gradients of the optimizer before starting backpropagation. Gradients need to be cleared to prevent accumulation from previous iterations.\n",
    "\n",
    "3. **Model Forward Pass**:\n",
    "   - **`vq_loss, data_recon, perplexity = model(data)`**: Runs a forward pass of the model. It returns the vector quantization loss, the reconstructed data, and the perplexity metric.\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - **`recon_error = F.mse_loss(data_recon, data) / data_variance`**: Calculates the Mean Squared Error (MSE) between the reconstructed data and the original data, normalized by the data variance. This normalization could help to standardize the error measure, making it less dependent on the scale of the dataset.\n",
    "   - **`loss = recon_error + vq_loss`**: Total loss is the sum of the reconstruction error and the VQ loss, combining these to form a single scalar to optimize.\n",
    "\n",
    "5. **Backpropagation and Parameter Update**:\n",
    "   - **`loss.backward()`**: Computes the gradients of the loss with respect to the model parameters.\n",
    "   - **`optimizer.step()`**: Updates the model parameters based on the computed gradients.\n",
    "\n",
    "### Logging and Tracking\n",
    "- **Appending Metrics**:\n",
    "  - **`train_res_recon_error.append(recon_error.item())`** and **`train_res_perplexity.append(perplexity.item())`**: Adds the current batch's reconstruction error and perplexity to the lists for later analysis.\n",
    "  \n",
    "- **Periodic Logging**:\n",
    "  - **`if (i+1) % 100 == 0:`**: Every 100 iterations, print statistics about the last 100 iterations:\n",
    "    - **`print('%d iterations' % (i+1))`**: Outputs the current iteration number.\n",
    "    - **`print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))`** and **`print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))`**: Computes and prints the mean of the last 100 recorded values for reconstruction error and perplexity, providing insight into the model's recent performance.\n",
    "\n",
    "### Summary\n",
    "This training loop is well-structured for models involving complex loss computations like those found in VQ-VAE, which need to handle both reconstruction quality and the properties of the latent space. \n",
    "\n",
    "**Question: Point out these lines in code:** The loop efficiently handles data loading, loss computation, backpropagation, parameter updates, and periodic logging, making it suitable for extensive training sessions over large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbe533-de7c-4bc9-a1cf-9dcee5bddd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in xrange(num_training_updates):\n",
    "    (data, _) = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    vq_loss, data_recon, perplexity = model(data)\n",
    "    recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "    loss = recon_error + vq_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('%d iterations' % (i+1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce999a-cb47-439f-a2cf-58906e4ec835",
   "metadata": {},
   "source": [
    "In the following, we would like to smooth the error so that plots dont look scary. For this we will use *Savitzky-Golay filter*. \n",
    "\n",
    "The savgol_filter function in this context refers to the Savitzky-Golay filter, a digital filter used for smoothing a set of digital data points. It is commonly used in signal processing to \"smooth\" noisy data, while preserving the shape and features of the signal, such as relative maxima, minima, and width. Let's break down the parameters of the savgol_filter function:\n",
    "\n",
    "* **Input Data (train_res_recon_error):** This is the data array that you want to smooth using the filter. It typically represents a series of measurements or signal values.\n",
    "\n",
    "* **Window Length (201):** This is the size of the filter window, meaning the filter will take into account 201 data points around each point to compute a new smoothed value. The window length must be a positive odd integer.\n",
    "\n",
    "* **Polynomial Order (7):** This specifies the order of the polynomial used to fit the samples within the window. The polynomial order must be less than the window length. A higher polynomial order can fit the windowed samples with a more complex curve, which can be useful for capturing finer details in the data.\n",
    "\n",
    "In use, the savgol_filter function will apply a polynomial smoothing to the train_res_recon_error data array over a window of 201 points, fitting a 7th-degree polynomial to the data within each window. This can help reduce noise and make trends in the data more apparent, while maintaining the integrity of the signal's high-frequency components.\n",
    "\n",
    "Reference: [Click here](https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d28b05a-7815-4de2-8a70-8fe3591da442",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
    "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6d5bd-4630-42d1-9043-e078d49beed4",
   "metadata": {},
   "source": [
    "The code snippet you see below `ax = f.add_subplot(1,2,1)` is typically used in Python's Matplotlib library, a popular plotting library for creating static, interactive, and animated visualizations in Python. Here's what each part of this snippet means:\n",
    "\n",
    "1. **`f`**: This variable is a figure object in Matplotlib, which serves as a container for all plot elements. A figure object can contain one or more subplots (axes), which are actual plots.\n",
    "\n",
    "2. **`add_subplot(1, 2, 1)`**: This method is used to add a subplot to the figure `f`. The method's arguments `(1, 2, 1)` define how the subplots are arranged and which subplot is being referred to:\n",
    "   - The first argument (`1`) indicates the number of rows of subplots the figure should have.\n",
    "   - The second argument (`2`) indicates the number of columns of subplots the figure should have.\n",
    "   - The third argument (`1`) is the index of the subplot to create or activate. Subplot indices start at 1 (not 0), and increase across rows and then down columns.\n",
    "\n",
    "   In this specific case, `add_subplot(1, 2, 1)` means \"create a subplot in a grid that has 1 row and 2 columns, and activate the first (left) subplot of this grid.\" \n",
    "\n",
    "As a result of this line of code, `ax` is now an AxesSubplot object that represents the left subplot of a figure that will contain exactly two side-by-side subplots. You can then use `ax` to plot graphs, set plot titles, labels, and other plot elements.\n",
    "\n",
    "**Quiz (Plotting in Python)**\n",
    "\n",
    "* What does ax = f.add_subplot(1,2,1) do in the following code?\n",
    "* Write code line (only write here) to add plot at (5,2) grid position in a (10,10) grid. Here each grid point refers to a figure plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75da45-1253-47ca-820c-3c725d3ccb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16,8))\n",
    "ax = f.add_subplot(1,2,1)\n",
    "ax.plot(train_res_recon_error_smooth)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Smoothed NMSE.')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax = f.add_subplot(1,2,2)\n",
    "ax.plot(train_res_perplexity_smooth)\n",
    "ax.set_title('Smoothed Average codebook usage (perplexity).')\n",
    "ax.set_xlabel('iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d31d7-c4dd-4b9b-8f3d-8077af6a3bc2",
   "metadata": {},
   "source": [
    "# Model.eval()\n",
    "\n",
    "The `model.eval()` method is used in many machine learning frameworks, such as PyTorch, to set a model into evaluation mode. This is important when you are no longer training the model, but want to assess its performance on validation or test data. Here's what happens and why it's important:\n",
    "\n",
    "1. **Disables Training Specific Behaviors**: Many models, especially in deep learning, behave differently during training and testing. For instance, layers like dropout and batch normalization function differently:\n",
    "   - **Dropout**: During training, dropout randomly zeroes some of the elements of the input tensor with a probability p, which is a technique for preventing overfitting. During evaluation, dropout is disabled, and all features are used (no elements are zeroed).\n",
    "   - **Batch Normalization**: During training, this layer normalizes its output using the mean and standard deviation of the current batch of inputs. For evaluation, it uses the running averages of these statistics, which were learned during training. \n",
    "\n",
    "2. **Consistency**: Using `model.eval()` ensures that the model's forward passes are consistent and not dependent on any random behavior specified during the training phase, such as dropout. This is crucial for getting reliable and reproducible results during model evaluation.\n",
    "\n",
    "3. **Performance Evaluation**: It prepares the model for performance evaluation or inference on unseen data. It tells the model that it should not expect any further changes to its parameters and that it should use its learned parameters as they are.\n",
    "\n",
    "In short, the `model.eval()` method is crucial for correctly and fairly assessing the performance of a neural network or any model that contains elements with different behaviors during training versus testing. This method helps to ensure that the model's predictions are based on learned patterns rather than artifacts of the training environment.\n",
    "\n",
    "**Quiz (machine learning) (5 marks)**\n",
    "* What is dropout in machine learning? Write in less than 5 lines. \n",
    "* What is Batch normalization in machine learning. Write in less than 5 lines.\n",
    "* What are random behaviours in a forward pass discussed above?\n",
    "* What happens to batch normalization layers during model.eval()?\n",
    "* Does model parameters get updated during model.eval() ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65205f7f-6f3f-416d-993e-b6389639c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "(valid_originals, _) = next(iter(validation_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n",
    "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "valid_reconstructions = model._decoder(valid_quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51d990ce-e1ab-4914-a679-8766f1655e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_originals, _) = next(iter(training_loader))\n",
    "train_originals = train_originals.to(device)\n",
    "_, train_reconstructions, _, _ = model._vq_vae(train_originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c37b512-7192-481d-8656-7184319fe291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaecc38-2755-4877-904c-d6c2c3102a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_grid(valid_reconstructions.cpu().data)+0.5, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92805244-59aa-4552-bb27-ea3ceb66dd7b",
   "metadata": {},
   "source": [
    "**Question:** Are you happy with the image quality? If not, then run this for larger epochs, the quality is expected to improve. \n",
    "Try num of training updates: 500, 1000, 1500, 5000, 10000 (if possible!) and save figures and notebook and upload in moodle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a277614-bf99-4f9e-af8a-d601c0faa869",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_grid(valid_originals.cpu()+0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92af41-8c35-43a9-8159-f9af7d9eceaa",
   "metadata": {},
   "source": [
    "# Projection and Visualization of VQ-VAE Embeddings\n",
    "\n",
    "The code snippet below is using UMAP (Uniform Manifold Approximation and Projection), a dimensionality reduction technique, to visualize or analyze high-dimensional data in a lower-dimensional space. \n",
    "\n",
    "UMAP paper: [paper](https://arxiv.org/abs/1802.03426)\n",
    "\n",
    "t-SNE based visualization: [paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "\n",
    "UMAP is particularly popular for its ability to preserve both the local and global structure of the data, making it highly suitable for tasks such as visualization of complex datasets, including embeddings in machine learning. Let's break down the elements of this code:\n",
    "\n",
    "### UMAP Parameters\n",
    "- **`n_neighbors=3`**: This parameter controls how UMAP balances local versus global structure in the data. It determines the number of neighboring points used in the local manifold approximation. A smaller number makes UMAP focus more on the local structure, while a larger value helps capture more of the global structure.\n",
    "\n",
    "- **`min_dist=0.1`**: This parameter sets the minimum distance between points in the low-dimensional projection. The smaller the value, the closer UMAP will try to pack points together, which can be useful for emphasizing clusters or local groupings in the data.\n",
    "\n",
    "- **`metric='cosine'`**: This specifies the metric used to measure distance in the input space. The cosine similarity is a common choice for high-dimensional data, especially when dealing with text or other types of data where the magnitude of the data vectors is not as important as their direction or angle.\n",
    "\n",
    "### Application to Model Embeddings\n",
    "- **`model._vq_vae._embedding.weight.data.cpu()`**: This extracts the weights of the embeddings from a Vector Quantizer within a VQ-VAE model. The embeddings represent the learned discrete representations of the input data. \n",
    "  - **`model._vq_vae._embedding`**: Refers to the embedding layer of the vector quantizer used in the VQ-VAE model.\n",
    "  - **`weight.data`**: Retrieves the actual parameters (weights) of the embedding layer, which are the focus of the dimensionality reduction.\n",
    "  - **`cpu()`**: Ensures that the tensor containing these weights is moved to the CPU. This is necessary because UMAP and many other non-PyTorch libraries do not support CUDA tensors (tensors on GPU).\n",
    "\n",
    "### UMAP Execution\n",
    "- **`umap.UMAP(...).fit_transform(...)`**: This function call does two things:\n",
    "  1. **`fit`**: UMAP fits to the provided high-dimensional data, learning how it can best be represented in lower dimensions.\n",
    "  2. **`transform`**: Transforms the high-dimensional data into the lower-dimensional space according to the fit.\n",
    "\n",
    "### Use Cases\n",
    "The resulting projection `proj` can be used for various purposes such as:\n",
    "- **Visualization**: The low-dimensional space can be plotted, typically in 2D or 3D, to visualize the structure of the data and discover patterns such as clusters.\n",
    "- **Analysis**: The projections can be analyzed to understand relationships and distances between different points in the embedding space, which can provide insights into how well the VQ-VAE model is capturing the variations in the data.\n",
    "\n",
    "In summary, this code effectively applies UMAP to reduce the dimensionality of the embedding weights from a VQ-VAE model. The aim is to aid in visualizing or analyzing the embeddings, which can be crucial for understanding and interpreting the behavior of complex machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c33b985a-d926-4906-ab3e-82647fdd2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = umap.UMAP(n_neighbors=3,\n",
    "                 min_dist=0.1,\n",
    "                 metric='cosine').fit_transform(model._vq_vae._embedding.weight.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbac45b-e39b-4628-b9e2-30e24e15e35e",
   "metadata": {},
   "source": [
    "**Question:** Write your observation after looking at the plot below. Save the figure and upload alongwith this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765ae48-a105-4a12-9279-432c294fff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea0949-e8b0-4b7f-8599-e740b37f95b0",
   "metadata": {},
   "source": [
    "**Question:** Implement t-SNE based visualization for these data: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html and show generated figure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
